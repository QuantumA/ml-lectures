{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import time\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyprind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9822/2480140289.py:42: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append([[txt, labels[l]]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:20\n"
     ]
    }
   ],
   "source": [
    "source = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "target = 'aclImdb_v1.tar.gz'\n",
    "\n",
    "if os.path.exists(target):\n",
    "    os.remove(target)\n",
    "\n",
    "\n",
    "def reporthook(count, block_size, total_size):\n",
    "    global start_time\n",
    "    if count == 0:\n",
    "        start_time = time.time()\n",
    "        return\n",
    "    duration = time.time() - start_time\n",
    "    progress_size = int(count * block_size)\n",
    "    speed = progress_size / (1024. ** 2 * duration)\n",
    "    percent = count * block_size * 100. / total_size\n",
    "\n",
    "    sys.stdout.write(f'\\r{int(percent)}% | {progress_size / (1024. ** 2):.2f} MB '\n",
    "                     f'| {speed:.2f} MB/s | {duration:.2f} sec elapsed')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "if not os.path.isdir('aclImdb') and not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "    urllib.request.urlretrieve(source, target, reporthook)\n",
    "\n",
    "if not os.path.isdir('aclImdb'):\n",
    "    with tarfile.open(target, 'r:gz') as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "basepath = 'aclImdb'\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file),\n",
    "                      'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]],\n",
    "                           ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']\n",
    "\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "\n",
    "# the following is necessary on some computers:\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
      "1  OK... so... I really like Kris Kristofferson a...          0\n",
      "2  ***SPOILER*** Do not read this, if you think a...          0\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.head(3))\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bag of Words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "{'in': 166,\n '1974': 1,\n 'the': 320,\n 'teenager': 315,\n 'martha': 200,\n 'moxley': 217,\n 'maggie': 197,\n 'grace': 139,\n 'moves': 214,\n 'to': 333,\n 'high': 153,\n 'class': 59,\n 'area': 22,\n 'of': 231,\n 'belle': 36,\n 'haven': 148,\n 'greenwich': 142,\n 'connecticut': 66,\n 'on': 234,\n 'mischief': 205,\n 'night': 226,\n 'eve': 106,\n 'halloween': 144,\n 'she': 289,\n 'was': 359,\n 'murdered': 220,\n 'backyard': 31,\n 'her': 151,\n 'house': 161,\n 'and': 16,\n 'murder': 219,\n 'remained': 266,\n 'unsolved': 347,\n 'twenty': 341,\n 'two': 342,\n 'years': 381,\n 'later': 188,\n 'writer': 379,\n 'mark': 199,\n 'fuhrman': 126,\n 'christopher': 57,\n 'meloni': 203,\n 'who': 371,\n 'is': 174,\n 'former': 122,\n 'la': 185,\n 'detective': 80,\n 'that': 319,\n 'has': 146,\n 'fallen': 112,\n 'disgrace': 88,\n 'for': 121,\n 'perjury': 245,\n 'simpson': 292,\n 'trial': 336,\n 'moved': 213,\n 'idaho': 164,\n 'decides': 76,\n 'investigate': 171,\n 'case': 51,\n 'with': 374,\n 'his': 155,\n 'partner': 241,\n 'stephen': 304,\n 'weeks': 365,\n 'andrew': 17,\n 'mitchell': 207,\n 'purpose': 257,\n 'writing': 380,\n 'book': 39,\n 'locals': 192,\n 'squirm': 302,\n 'do': 89,\n 'not': 229,\n 'welcome': 366,\n 'them': 322,\n 'but': 46,\n 'support': 313,\n 'retired': 269,\n 'steve': 305,\n 'carroll': 50,\n 'robert': 272,\n 'forster': 123,\n 'charge': 54,\n 'investigation': 172,\n '70': 3,\n 'they': 324,\n 'discover': 87,\n 'criminal': 73,\n 'net': 224,\n 'power': 251,\n 'money': 209,\n 'cover': 70,\n 'br': 41,\n 'good': 135,\n 'tv': 340,\n 'movie': 215,\n 'true': 338,\n 'story': 306,\n 'fifteen': 118,\n 'old': 233,\n 'girl': 129,\n 'committed': 61,\n 'by': 47,\n 'wealthy': 364,\n 'whose': 372,\n 'mother': 211,\n 'kennedy': 181,\n 'powerful': 252,\n 'rich': 270,\n 'family': 113,\n 'used': 349,\n 'their': 321,\n 'influence': 168,\n 'more': 210,\n 'than': 317,\n 'however': 163,\n 'snoopy': 295,\n 'convicted': 67,\n 'perjurer': 244,\n 'able': 4,\n 'disclose': 86,\n 'how': 162,\n 'hideous': 152,\n 'crime': 72,\n 'screenplay': 280,\n 'shows': 291,\n 'last': 187,\n 'days': 75,\n 'parallel': 240,\n 'there': 323,\n 'lack': 186,\n 'emotion': 101,\n 'dramatization': 94,\n 'my': 222,\n 'vote': 357,\n 'seven': 287,\n 'title': 332,\n 'brazil': 42,\n 'available': 29,\n 'ok': 232,\n 'so': 296,\n 'really': 262,\n 'like': 189,\n 'kris': 182,\n 'kristofferson': 183,\n 'usual': 350,\n 'easy': 98,\n 'going': 133,\n 'delivery': 77,\n 'lines': 190,\n 'movies': 216,\n 'age': 10,\n 'helped': 150,\n 'him': 154,\n 'soft': 297,\n 'spoken': 301,\n 'low': 195,\n 'energy': 103,\n 'style': 308,\n 'he': 149,\n 'will': 373,\n 'steal': 303,\n 'scene': 278,\n 'effortlessly': 99,\n 'disappearance': 85,\n 'misstep': 206,\n 'holy': 157,\n 'moly': 208,\n 'this': 327,\n 'bad': 32,\n 'must': 221,\n 'give': 130,\n 'kudos': 184,\n 'cinematography': 58,\n 'actors': 8,\n 'including': 167,\n 'trying': 339,\n 'darndest': 74,\n 'make': 198,\n 'sense': 286,\n 'from': 124,\n 'goofy': 138,\n 'confusing': 65,\n 'none': 227,\n 'it': 175,\n 'made': 196,\n 'probably': 254,\n 'didn': 81,\n 'understand': 346,\n 'either': 100,\n 'just': 180,\n 'through': 328,\n 'motions': 212,\n 'hoping': 159,\n 'someone': 299,\n 'would': 378,\n 'come': 60,\n 'up': 348,\n 'tell': 316,\n 'what': 367,\n 'all': 12,\n 'about': 5,\n 'don': 92,\n 'care': 49,\n 'everyone': 109,\n 'doing': 91,\n 'out': 239,\n 'love': 194,\n 'project': 256,\n 'or': 238,\n 'some': 298,\n 'such': 311,\n 'nonsense': 228,\n 've': 352,\n 'seen': 284,\n 'budget': 45,\n 'had': 143,\n 'plot': 250,\n 'goodness': 137,\n 'sake': 276,\n 'zilcho': 386,\n 'nada': 223,\n 'zippo': 387,\n 'empty': 102,\n 'reason': 263,\n 'complete': 62,\n 'waste': 360,\n 'talent': 314,\n 'scenery': 279,\n 'celluloid': 52,\n 'rented': 267,\n 'piece': 246,\n 'garbage': 128,\n 'buck': 44,\n 'want': 358,\n 'back': 30,\n 'hours': 160,\n 'invested': 170,\n 'grade': 140,\n 'time': 331,\n 'watch': 361,\n 'minute': 204,\n 'your': 385,\n 'valuable': 351,\n 'while': 370,\n 'passing': 242,\n 'room': 274,\n 'where': 368,\n 'playing': 249,\n 'even': 107,\n 'open': 237,\n 'holding': 156,\n 'dvd': 97,\n 'believe': 35,\n 'me': 202,\n 'you': 383,\n 'll': 191,\n 'thank': 318,\n 'advice': 9,\n 'spoiler': 300,\n 'read': 261,\n 'if': 165,\n 'think': 326,\n 'watching': 362,\n 'although': 13,\n 'be': 33,\n 'way': 363,\n 'predictable': 253,\n 'does': 90,\n 'any': 19,\n 'difference': 82,\n 'anyway': 20,\n 'are': 21,\n 'wondering': 376,\n 'whether': 369,\n 'see': 283,\n 'coyote': 71,\n 'ugly': 344,\n 'worth': 377,\n 'ticket': 330,\n 'vhs': 355,\n 'typical': 343,\n 'chick': 56,\n 'feel': 116,\n 'flick': 120,\n 'one': 235,\n 'could': 69,\n 'say': 277,\n 'itself': 176,\n 'as': 24,\n 'shallow': 288,\n 'can': 48,\n 'ridiculous': 271,\n 'uncritical': 345,\n 'version': 353,\n 'american': 14,\n 'dream': 95,\n 'young': 384,\n 'looking': 193,\n 'small': 294,\n 'town': 334,\n 'becoming': 34,\n 'big': 38,\n 'success': 310,\n 'new': 225,\n 'york': 382,\n 'few': 117,\n 'desperate': 79,\n 'attempts': 27,\n 'giving': 131,\n 'depth': 78,\n 'fail': 110,\n 'tragic': 335,\n 'accident': 6,\n 'father': 114,\n 'difficulties': 83,\n 'violet': 356,\n 'relationship': 264,\n 'boyfriend': 40,\n 'mcnally': 201,\n 'director': 84,\n 'tries': 337,\n 'arouse': 23,\n 'audience': 28,\n 'pity': 248,\n 'sadness': 275,\n 'put': 258,\n 'have': 147,\n 'chance': 53,\n 'succeed': 309,\n 'attempt': 26,\n 'due': 96,\n 'script': 281,\n 'acting': 7,\n 'especially': 105,\n 'piper': 247,\n 'perabo': 243,\n 'completely': 63,\n 'fails': 111,\n 'convincing': 68,\n 'jersey': 178,\n 'fear': 115,\n 'singing': 293,\n 'front': 125,\n 'an': 15,\n 'only': 236,\n 'quite': 259,\n 'funny': 127,\n 'thing': 325,\n 'john': 179,\n 'goodman': 136,\n 'represents': 268,\n 'ray': 260,\n 'hope': 158,\n 'very': 354,\n 'astonished': 25,\n 'jerry': 177,\n 'bruckheimer': 43,\n 'produced': 255,\n 'first': 119,\n 'gone': 134,\n '60': 2,\n 'seconds': 282,\n 'now': 230,\n 'happened': 145,\n 'great': 141,\n 'rock': 273,\n 'con': 64,\n 'air': 11,\n 'stuff': 307,\n 'superficial': 312,\n 'women': 375,\n 'relaxed': 265,\n 'evening': 108,\n 'should': 290,\n 'better': 37,\n 'go': 132,\n 'charlie': 55,\n 'angels': 18,\n 'much': 218,\n 'entertaining': 104,\n 'self': 285,\n 'ironic': 173,\n 'instead': 169,\n 'thumbs': 329,\n 'down': 93,\n '10': 0}"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "docs = count.fit_transform(raw_documents=df[\"review\"][:3])\n",
    "\n",
    "count.vocabulary_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 1, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 1, 1, 1],\n       [1, 0, 1, ..., 0, 0, 0]])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TF-IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.  , 0.04, 0.  , ..., 0.  , 0.  , 0.  ],\n       [0.  , 0.  , 0.  , ..., 0.05, 0.05, 0.05],\n       [0.04, 0.  , 0.04, ..., 0.  , 0.  , 0.  ]])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "tfidf.fit_transform(count.fit_transform(df[\"review\"][:3])).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cleaning text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(t):\n",
    "    text = re.sub('<[^>]*>', '', t)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text)\n",
    "\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) +\n",
    "            ' '.join(emoticons).replace('-', ''))\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "df['review'] = df['review'].apply(preprocess_text)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenizing documents"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['runner', 'like', 'run', 'and', 'thu', 'they', 'run']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(t):\n",
    "    return [porter.stem(word) for word in t.split()]\n",
    "\n",
    "tokenizer_porter('runners like running and thus they run')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/acp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "['runner', 'like', 'run', 'run', 'lot']"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training a logistic regression model for document classification"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "X_train, y_train = df.loc[:25000, 'review'].values, df.loc[:25000, 'sentiment'].values\n",
    "X_test, y_test = df.loc[25000:, 'review'].values, df.loc[25000:, 'sentiment'].values"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": "GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[('vect',\n                                        TfidfVectorizer(lowercase=False)),\n                                       ('clf',\n                                        LogisticRegression(solver='liblinear'))]),\n             n_jobs=-1,\n             param_grid=[{'clf__C': [1.0, 10.0], 'clf__penalty': ['l2'],\n                          'vect__ngram_range': [(1, 1)],\n                          'vect__stop_words': [None],\n                          'vect__tokenizer': [<function tokenizer at 0x7f174f0ec700>,\n                                              <function tokenizer_porter at 0x7f174f0ec790...\n                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n                                                'our', 'ours', 'ourselves',\n                                                'you', \"you're\", \"you've\",\n                                                \"you'll\", \"you'd\", 'your',\n                                                'yours', 'yourself',\n                                                'yourselves', 'he', 'him',\n                                                'his', 'himself', 'she',\n                                                \"she's\", 'her', 'hers',\n                                                'herself', 'it', \"it's\", 'its',\n                                                'itself', ...],\n                                               None],\n                          'vect__tokenizer': [<function tokenizer at 0x7f174f0ec700>],\n                          'vect__use_idf': [False]}],\n             scoring='accuracy', verbose=1)",
      "text/html": "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n                                        TfidfVectorizer(lowercase=False)),\n                                       (&#x27;clf&#x27;,\n                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n             n_jobs=-1,\n             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n                          &#x27;vect__stop_words&#x27;: [None],\n                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7f174f0ec700&gt;,\n                                              &lt;function tokenizer_porter at 0x7f174f0ec790...\n                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n                                                &#x27;itself&#x27;, ...],\n                                               None],\n                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7f174f0ec700&gt;],\n                          &#x27;vect__use_idf&#x27;: [False]}],\n             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n             estimator=Pipeline(steps=[(&#x27;vect&#x27;,\n                                        TfidfVectorizer(lowercase=False)),\n                                       (&#x27;clf&#x27;,\n                                        LogisticRegression(solver=&#x27;liblinear&#x27;))]),\n             n_jobs=-1,\n             param_grid=[{&#x27;clf__C&#x27;: [1.0, 10.0], &#x27;clf__penalty&#x27;: [&#x27;l2&#x27;],\n                          &#x27;vect__ngram_range&#x27;: [(1, 1)],\n                          &#x27;vect__stop_words&#x27;: [None],\n                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7f174f0ec700&gt;,\n                                              &lt;function tokenizer_porter at 0x7f174f0ec790...\n                          &#x27;vect__stop_words&#x27;: [[&#x27;i&#x27;, &#x27;me&#x27;, &#x27;my&#x27;, &#x27;myself&#x27;, &#x27;we&#x27;,\n                                                &#x27;our&#x27;, &#x27;ours&#x27;, &#x27;ourselves&#x27;,\n                                                &#x27;you&#x27;, &quot;you&#x27;re&quot;, &quot;you&#x27;ve&quot;,\n                                                &quot;you&#x27;ll&quot;, &quot;you&#x27;d&quot;, &#x27;your&#x27;,\n                                                &#x27;yours&#x27;, &#x27;yourself&#x27;,\n                                                &#x27;yourselves&#x27;, &#x27;he&#x27;, &#x27;him&#x27;,\n                                                &#x27;his&#x27;, &#x27;himself&#x27;, &#x27;she&#x27;,\n                                                &quot;she&#x27;s&quot;, &#x27;her&#x27;, &#x27;hers&#x27;,\n                                                &#x27;herself&#x27;, &#x27;it&#x27;, &quot;it&#x27;s&quot;, &#x27;its&#x27;,\n                                                &#x27;itself&#x27;, ...],\n                                               None],\n                          &#x27;vect__tokenizer&#x27;: [&lt;function tokenizer at 0x7f174f0ec700&gt;],\n                          &#x27;vect__use_idf&#x27;: [False]}],\n             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, TfidfVectorizer(lowercase=False)),\n                (&#x27;clf&#x27;, LogisticRegression(solver=&#x27;liblinear&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(lowercase=False)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "small_param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [None],\n",
    "                     'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                     'clf__C': [1.0, 10.0]},\n",
    "                    {'vect__ngram_range': [(1, 1)],\n",
    "                     'vect__stop_words': [stop, None],\n",
    "                     'vect__tokenizer': [tokenizer],\n",
    "                     'vect__use_idf':[False],\n",
    "                     'vect__norm':[None],\n",
    "                     'clf__penalty': ['l2'],\n",
    "                  'clf__C': [1.0, 10.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "{'clf__C': 10.0,\n 'clf__penalty': 'l2',\n 'vect__ngram_range': (1, 1),\n 'vect__stop_words': None,\n 'vect__tokenizer': <function __main__.tokenizer(text)>}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0.8970842631473704"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Out-of-core learning"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                           text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower() \\\n",
    "           + ' '.join(emoticons).replace('-', ''))\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv) # skip header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "('\"In 1974, the teenager Martha Moxley (Maggie Grace) moves to the high-class area of Belle Haven, Greenwich, Connecticut. On the Mischief Night, eve of Halloween, she was murdered in the backyard of her house and her murder remained unsolved. Twenty-two years later, the writer Mark Fuhrman (Christopher Meloni), who is a former LA detective that has fallen in disgrace for perjury in O.J. Simpson trial and moved to Idaho, decides to investigate the case with his partner Stephen Weeks (Andrew Mitchell) with the purpose of writing a book. The locals squirm and do not welcome them, but with the support of the retired detective Steve Carroll (Robert Forster) that was in charge of the investigation in the 70\\'s, they discover the criminal and a net of power and money to cover the murder.<br /><br />\"\"Murder in Greenwich\"\" is a good TV movie, with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a Kennedy. The powerful and rich family used their influence to cover the murder for more than twenty years. However, a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed. The screenplay shows the investigation of Mark and the last days of Martha in parallel, but there is a lack of the emotion in the dramatization. My vote is seven.<br /><br />Title (Brazil): Not Available\"',\n 1)"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "next(stream_docs(path='movie_data.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nUnfortunately, we can’t use CountVectorizer for out-of-core learning since it requires holding the\\ncomplete vocabulary in memory. Also, TfidfVectorizer needs to keep all the feature vectors of the\\ntraining dataset in memory to calculate the inverse document frequencies. However, another useful\\nvectorizer for text processing implemented in scikit-learn is HashingVectorizer. HashingVectorizer\\nis data-independent and makes use of the hashing trick via the 32-bit MurmurHash3 function by Austin\\nAppleby (you can find more information about MurmurHash at https://en.wikipedia.org/wiki/MurmurHash)\\n'"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Unfortunately, we can’t use CountVectorizer for out-of-core learning since it requires holding the\n",
    "complete vocabulary in memory. Also, TfidfVectorizer needs to keep all the feature vectors of the\n",
    "training dataset in memory to calculate the inverse document frequencies. However, another useful\n",
    "vectorizer for text processing implemented in scikit-learn is HashingVectorizer. HashingVectorizer\n",
    "is data-independent and makes use of the hashing trick via the 32-bit MurmurHash3 function by Austin\n",
    "Appleby (you can find more information about MurmurHash at https://en.wikipedia.org/wiki/MurmurHash)\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.868\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "doc = get_minibatch(doc_stream, size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Topic modeling with latent Dirichlet allocation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LDA is a generative probabilistic model that tries to find groups of words that appear frequently together across different\n",
    "documents. These frequently appearing words represent our topics, assuming\n",
    "that each document is a mixture of different words. The input to an LDA is the bag-of-words model.\n",
    "\n",
    "Given a bag-of-words matrix as input, LDA decomposes it into two new matrices:\n",
    " • A document-to-topic matrix\n",
    " • A word-to-topic matrix\n",
    "\n",
    "LDA decomposes the bag-of-words matrix in such a way that if we multiply those two matrices together, we will be able to reproduce\n",
    "the input, the bag-of-words matrix, with the lowest possible error. In practice, we are interested in those topics that LDA found in the\n",
    "bag-of-words matrix. The only downside may be that we must define the number of topics beforehand—the number of topics is a\n",
    "hyperparameter of LDA that has to be specified manually."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LDA with scikit-learn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "# the following is necessary on some computers:\n",
    "df = df.rename(columns={\"0\": \"review\", \"1\": \"sentiment\"})\n",
    "\n",
    "count = CountVectorizer(stop_words='english',\n",
    "                        max_df=.1,\n",
    "                        max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10,\n",
    "                                random_state=123,\n",
    "                                learning_method='batch')\n",
    "X_topics = lda.fit_transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "data": {
      "text/plain": "(10, 5000)"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i]\n",
    "                    for i in topic.argsort()\\\n",
    "                        [:-n_top_words - 1:-1]]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
      "\n",
      "Horror movie #3:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300], '...')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}